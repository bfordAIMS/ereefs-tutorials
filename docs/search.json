[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AIMS eReefs tutorials",
    "section": "",
    "text": "Tutorial\nLanguages\n\n\n\n\nPlotting time series data\n R    Python\n\n\nAccessing eReefs data from the AIMS server\n R    Python\n\n\nProcessing eReefs data\n Python\n\n\nOr could have icons like this"
  },
  {
    "objectID": "tutorials/python/basic-time-series-plot-python/basic_time_series_plot_python.html",
    "href": "tutorials/python/basic-time-series-plot-python/basic_time_series_plot_python.html",
    "title": "Time series plots  Python",
    "section": "",
    "text": "In this notebook we use OpeNDAP to extract time series data at a single location of interest, then plot this data. This extraction process can also be done with the AIMS eReefs data extraction tool. If you which to perform bigger extractions then we recommend using this tool instead of this process outlined in this example.\n\nNote: This script has no error checking and so changing the date ranges or locations might result in out of bounds errors.\n\n\nLoad the required Python libraries\n\nfrom netCDF4 import Dataset, num2date\nimport matplotlib.pyplot as plt\nimport cartopy\nimport cartopy.crs as ccrs\nimport os\nimport datetime\nimport pandas as pd\nimport numpy as np\ncartopy.config['data_dir'] = os.getenv('CARTOPY_DIR', cartopy.config.get('data_dir'))\n\n\n\nChoose OPeNDAP end point\nThe first part of the process is to choose the OPeNDAP end point on the AIMS eReefs THREDD server. You can view the products here. At this stage there is no grouped OPeNDAP service for the entire time series and so this script only works for looking at a single month of data. Hopefully this can be improved in the future.\n\n# Connect to the OpeNDAP endpoint for the specified month.  \nmonth = 3\nyear = 2020\nnetCDF_datestr = str(year)+'-'+format(month, '02')\nnetCDF_datestr\n\n'2020-03'\n\n\n\n# OPeNDAP URL to file \"EREEFS_AIMS-CSIRO_gbr4_v2_hydro_daily-monthly-YYYY-MM.nc\". Hydrodynamic 4km model, daily data for the month specified\ninputFile = \"http://thredds.ereefs.aims.gov.au/thredds/dodsC/s3://aims-ereefs-public-prod/derived/ncaggregate/ereefs/gbr4_v2/daily-monthly/EREEFS_AIMS-CSIRO_gbr4_v2_hydro_daily-monthly-\"+netCDF_datestr+\".nc\"\n\nnc_data = Dataset(inputFile, 'r')\nprint(nc_data.title)\n\n# To find a list of the variables uncomment the next line: \nnc_data.variables\n\neReefs AIMS-CSIRO GBR4 Hydrodynamic v2 daily aggregation\n\n\n{'mean_cur': <class 'netCDF4._netCDF4.Variable'>\n float32 mean_cur(time, k, latitude, longitude)\n     coordinates: time zc latitude longitude\n     substanceOrTaxon_id: http://environment.data.gov.au/def/feature/ocean_current\n     units: ms-1\n     puv__parameter: http://vocab.nerc.ac.uk/collection/P01/current/LCEWMP01/\n     medium_id: http://environment.data.gov.au/def/feature/ocean\n     unit_id: http://qudt.org/vocab/unit#MeterPerSecond\n     short_name: mean_cur\n     aggregation: mean_speed\n     standard_name: mean_current_speed\n     long_name: mean_current_speed\n     _ChunkSizes: [  1   1 133 491]\n unlimited dimensions: time\n current shape = (31, 17, 723, 491)\n filling off,\n 'salt': <class 'netCDF4._netCDF4.Variable'>\n float32 salt(time, k, latitude, longitude)\n     qudt__unit: http://qudt.org/vocab/unit/PSU\n     puv__parameter: http://vocab.nerc.ac.uk/collection/P01/current/PSLTMP01/\n     coordinates: time zc latitude longitude\n     substanceOrTaxon_id: http://sweet.jpl.nasa.gov/2.2/matrWater.owl#SaltWater\n     scaledQuantityKind_id: http://environment.data.gov.au/def/property/practical_salinity\n     short_name: salt\n     aggregation: Daily\n     units: PSU\n     medium_id: http://environment.data.gov.au/def/feature/ocean\n     unit_id: http://environment.data.gov.au/water/quality/def/unit/PSU\n     long_name: Salinity\n     _ChunkSizes: [  1   1 133 491]\n unlimited dimensions: time\n current shape = (31, 17, 723, 491)\n filling off,\n 'temp': <class 'netCDF4._netCDF4.Variable'>\n float32 temp(time, k, latitude, longitude)\n     puv__parameter: https://vocab.nerc.ac.uk/collection/P01/current/TEMPMP01/\n     coordinates: time zc latitude longitude\n     substanceOrTaxon_id: http://sweet.jpl.nasa.gov/2.2/matrWater.owl#SaltWater\n     scaledQuantityKind_id: http://environment.data.gov.au/def/property/sea_water_temperature\n     short_name: temp\n     aggregation: Daily\n     units: degrees C\n     medium_id: http://environment.data.gov.au/def/feature/ocean\n     unit_id: http://qudt.org/vocab/unit#DegreeCelsius\n     long_name: Temperature\n     _ChunkSizes: [  1   1 133 491]\n unlimited dimensions: time\n current shape = (31, 17, 723, 491)\n filling off,\n 'u': <class 'netCDF4._netCDF4.Variable'>\n float32 u(time, k, latitude, longitude)\n     vector_components: u v\n     coordinates: time zc latitude longitude\n     substanceOrTaxon_id: http://environment.data.gov.au/def/feature/ocean_current\n     vector_name: Currents\n     aggregation: Daily\n     units: ms-1\n     long_name: Eastward current\n     puv__parameter: http://vocab.nerc.ac.uk/collection/P01/current/LCEWMP01/\n     scaledQuantityKind_id: http://environment.data.gov.au/def/property/sea_water_velocity_eastward\n     short_name: u\n     standard_name: eastward_sea_water_velocity\n     medium_id: http://environment.data.gov.au/def/feature/ocean\n     unit_id: http://qudt.org/vocab/unit#MeterPerSecond\n     _ChunkSizes: [  1   1 133 491]\n unlimited dimensions: time\n current shape = (31, 17, 723, 491)\n filling off,\n 'v': <class 'netCDF4._netCDF4.Variable'>\n float32 v(time, k, latitude, longitude)\n     vector_components: u v\n     coordinates: time zc latitude longitude\n     substanceOrTaxon_id: http://environment.data.gov.au/def/feature/ocean_current\n     vector_name: Currents\n     aggregation: Daily\n     units: ms-1\n     long_name: Northward current\n     puv__parameter: http://vocab.nerc.ac.uk/collection/P01/current/LCNSMP01/\n     scaledQuantityKind_id: http://environment.data.gov.au/def/property/sea_water_velocity_northward\n     short_name: v\n     standard_name: northward_sea_water_velocity\n     medium_id: http://environment.data.gov.au/def/feature/ocean\n     unit_id: http://qudt.org/vocab/unit#MeterPerSecond\n     _ChunkSizes: [  1   1 133 491]\n unlimited dimensions: time\n current shape = (31, 17, 723, 491)\n filling off,\n 'zc': <class 'netCDF4._netCDF4.Variable'>\n float64 zc(k)\n     positive: up\n     coordinate_type: Z\n     units: m\n     long_name: Z coordinate\n     axis: Z\n     _CoordinateAxisType: Height\n     _CoordinateZisPositive: up\n unlimited dimensions: \n current shape = (17,)\n filling off,\n 'time': <class 'netCDF4._netCDF4.Variable'>\n float64 time(time)\n     units: days since 1990-01-01 00:00:00 +10\n     long_name: Time\n     standard_name: time\n     coordinate_type: time\n     puv__uom: http://vocab.nerc.ac.uk/collection/P06/current/UTAA/\n     calendar: gregorian\n     _CoordinateAxisType: Time\n     _ChunkSizes: 1024\n unlimited dimensions: time\n current shape = (31,)\n filling off,\n 'latitude': <class 'netCDF4._netCDF4.Variable'>\n float64 latitude(latitude)\n     long_name: Latitude\n     standard_name: latitude\n     units: degrees_north\n     coordinate_type: latitude\n     projection: geographic\n     puv__ofProperty: http://vocab.nerc.ac.uk/collection/S06/current/S0600045/\n     puv__uom: http://vocab.nerc.ac.uk/collection/P06/current/DEGN/\n     _CoordinateAxisType: Lat\n unlimited dimensions: \n current shape = (723,)\n filling off,\n 'longitude': <class 'netCDF4._netCDF4.Variable'>\n float64 longitude(longitude)\n     standard_name: longitude\n     long_name: Longitude\n     units: degrees_east\n     puv__uom: http://vocab.nerc.ac.uk/collection/P06/current/DEGE/\n     coordinate_type: longitude\n     projection: geographic\n     _CoordinateAxisType: Lon\n unlimited dimensions: \n current shape = (491,)\n filling off,\n 'mean_wspeed': <class 'netCDF4._netCDF4.Variable'>\n float32 mean_wspeed(time, latitude, longitude)\n     puv__parameter: http://vocab.nerc.ac.uk/collection/P01/current/ESEWMPXX/\n     coordinates: time latitude longitude\n     units: ms-1\n     short_name: mean_wspeed\n     aggregation: mean_speed\n     standard_name: mean_wind_speed\n     long_name: mean_wind_speed\n     _ChunkSizes: [  1 133 491]\n unlimited dimensions: time\n current shape = (31, 723, 491)\n filling off,\n 'eta': <class 'netCDF4._netCDF4.Variable'>\n float32 eta(time, latitude, longitude)\n     puv__parameter: https://vocab.nerc.ac.uk/collection/P01/current/ASLVMP01/\n     coordinates: time latitude longitude\n     substanceOrTaxon_id: http://environment.data.gov.au/def/feature/ocean_near_surface\n     scaledQuantityKind_id: http://environment.data.gov.au/def/property/sea_surface_elevation\n     short_name: eta\n     standard_name: sea_surface_height_above_sea_level\n     aggregation: Daily\n     units: metre\n     positive: up\n     medium_id: http://environment.data.gov.au/def/feature/ocean\n     unit_id: http://qudt.org/vocab/unit#Meter\n     long_name: Surface elevation\n     _ChunkSizes: [  1 133 491]\n unlimited dimensions: time\n current shape = (31, 723, 491)\n filling off,\n 'wspeed_u': <class 'netCDF4._netCDF4.Variable'>\n float32 wspeed_u(time, latitude, longitude)\n     puv__parameter: http://vocab.nerc.ac.uk/collection/P01/current/ESEWMPXX/\n     coordinates: time latitude longitude\n     short_name: wspeed_u\n     aggregation: Daily\n     units: ms-1\n     long_name: eastward_wind\n     _ChunkSizes: [  1 133 491]\n unlimited dimensions: time\n current shape = (31, 723, 491)\n filling off,\n 'wspeed_v': <class 'netCDF4._netCDF4.Variable'>\n float32 wspeed_v(time, latitude, longitude)\n     puv__parameter: http://vocab.nerc.ac.uk/collection/P01/current/ESNSMPXX/\n     coordinates: time latitude longitude\n     short_name: wspeed_v\n     aggregation: Daily\n     units: ms-1\n     long_name: northward_wind\n     _ChunkSizes: [  1 133 491]\n unlimited dimensions: time\n current shape = (31, 723, 491)\n filling off}\n\n\n\n\nSelect the point location\nWork out the bounds of the gridded data. We can then use this to find out which grid cell best matches our location of interest.\n\nNote: This only works because the AIMS eReefs aggregate datasets are regridded onto a regularly spaced grid. The original raw model data is on a curvilinear grid and this approach would not work for that data.\n\n\nlons = nc_data.variables['longitude'][:]\nmax_lon = max(lons)\nmin_lon = min(lons) \nlats = nc_data.variables['latitude'][:]\nmax_lat = max(lats)\nmin_lat = min(lats)\ngrid_lon = lons.size\ngrid_lat = lats.size\nprint(\"Grid bounds, Lon: \"+str(min_lon)+\" - \"+str(max_lon)+\" Lat:\"+str(min_lat)+\" - \"+str(max_lat))\nprint(\"Grid size is: \"+str(grid_lon)+\" x \"+str(grid_lat))\n\nGrid bounds, Lon: 142.168788 - 156.868788 Lat:-28.696022 - -7.036022\nGrid size is: 491 x 723\n\n\nFind the closest index to the location of interest.\n\n# Davies reef\nlat = -18.82\nlon = 147.64\nselectedLatIndex = round((lat-min_lat)/(max_lat-min_lat)*grid_lat)\nselectedLonIndex = round((lon-min_lon)/(max_lon-min_lon)*grid_lon)\nprint(\"Grid position of location: \"+str(selectedLatIndex)+\", \"+str(selectedLonIndex))\n\nGrid position of location: 330, 183\n\n\n\n\nExtract values\nExtract the values over time at this location. Note that because we are access the underlying data here this results in an OpeNDAP call to get the data from the remote server. As a result this call can take a while (~10 sec).\n\nselectedDepthIndex = 15 # -1.5m\nselectedDepthIndex2 = 10 # -17.75m\n\n# Time, Depth, Lat, Lon\ndailyTemp1 = nc_data.variables['temp'][:,[selectedDepthIndex,selectedDepthIndex2], selectedLatIndex, selectedLonIndex]\nprint(dailyTemp1[0:5])\n\n[[29.586527 27.764503]\n [29.513176 27.867159]\n [29.505758 28.179167]\n [29.782936 28.216429]\n [30.008278 28.062304]]\n\n\nLet’s get the wind for the same location. The wind variable doesn’t have any depth dimension and so our indexing into the data is different. The wind is a vector measurement, with an x and y component.\n\nwspeed_v = nc_data.variables['wspeed_v'][:, selectedLatIndex, selectedLonIndex]\nwspeed_u = nc_data.variables['wspeed_v'][:, selectedLatIndex, selectedLonIndex]\n\nTo get the wind speed we need to calculate the magnitude of this vector.\n\nwspeed = np.sqrt(wspeed_v**2 + wspeed_u**2)\n\nGet the time series. Note that the time values are stored as the number of days since 1990-01-01 00:00:00 +10.\n\ntimes = nc_data.variables['time'][:]\nprint(times[0:5])\n\n[11017. 11018. 11019. 11020. 11021.]\n\n\n\n\nPlot the time series\n\n# Convert the days since the 1990 origin into Pandas dates for plotting\nt = pd.to_datetime(times,unit='D',origin=pd.Timestamp('1990-01-01'))\n\nfig, ax1 = plt.subplots()\nfig.set_size_inches(8, 7)\n\n\nax1.set_xlabel('Date')\nax1.set_ylabel('Temperature (deg C)')\nax1.plot(t, dailyTemp1[:,0], color='tab:red', label='Temp (-1.5 m)')\nax1.plot(t, dailyTemp1[:,1], color='tab:orange', label='Temp (-17.75 m)')\n#ax1.tick_params(axis='y', labelcolor=color)\n\nax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n\ncolor = 'tab:blue'\nax2.set_ylabel('Wind speed (m/s)', color=color)  # we already handled the x-label with ax1\nax2.plot(t, wspeed, color=color, label='Wind')\nax2.tick_params(axis='y', labelcolor=color)\n\nfig.legend()\n# Set the axes formating to show the dates on an angle on the current figure (gcf)\nplt.gcf().autofmt_xdate()\n\n#fig.tight_layout()  # otherwise the right y-label is slightly clipped\n\n\n\n\nFrom this graph we can see that the surface water at Davies Reef was very warm during March 2020. There was a strong stratification of the temperature profile with cool water at -18 m. Around the 10th March the wind picked up for a few days, mixing the water, cooling the surface down rapidly."
  },
  {
    "objectID": "tutorials/python/processing-data-python/processing_data_python.html",
    "href": "tutorials/python/processing-data-python/processing_data_python.html",
    "title": "Processing eReefs data ",
    "section": "",
    "text": "Motivating problem\nIn this example we use eReefs time series data to try answer the question:\nHow strong does the wind need to be to set the direction of the surface ocean currents?\nWe use the AIMS eReefs extraction tool to extract data for two locations of interest:\n\nMyrmidon Reef which is on the far outer edge of the GBR and is almost right in the middle of the southern Eastern Australian Current; and\nDavies Reef which is further in the reef matrix, but in a similar sector of the GBR.\n\nWe then process the data and investigate the relationship between the strength of the wind and the direction of the surface currents for two locations.\n\n\nAnalysis method\nThe East Australian Current (EAC) usually is a strong southward current near Myrmidon and Davies Reefs. During winter months the wind moves in a north eastern direction in the near opposite direction to the EAC. When the wind is low the surface currents are dominated by the EAC. As the wind picks up at some speed the wind overpowers the EAC and the surface current moves in the direction of the wind.\nTo determine the relation between the wind and the surface currents we will use the AIMS eReefs extraction tool to pull out hourly time series wind and current data for our two locations of interest. We will then look at the correlation between the wind and current vectors, where a correlation of 1 indicates they are pointing in the same direction, and -1 indicated they are in opposite directions.\n\n\nSetting up to get the data\nTo extract the time series data using the extraction tool we need to create a CSV file containing the sites of interest. This file needs to contain the coordinates and names of the sites. To create this I first added my points manually in Google Earth Pro. This was done to simply get the location of Myrmidon and Davies Reefs. Using Google Earth to create your CSV file for the extraction tool is only useful if you don’t already know the coordinates of your sites.\nScreenshot of Google Earth Pro with Myromidon and Davies reef sites\nThe points can be added using the Add placemark tool (looks like a pin). The locations can be seen by displaying the placemark properties. The resulting KML file can be found here.\nThe location of the two sites were copied to create the CSV file for the data extraction tool.\n\n\nExtracting the data\nThe CSV file was uploaded to the AIMS extraction tool and the extraction was performed with the following settings:\n\nData collection: GBR1 Hydro (Version 2)\nVariables:\n\nEastward wind speed (wspeed_u)\nNorthward wind speed (wspeed_v)\nNorthward current (v)\nEastward current (u)\n\nDate range: 1 January 2019 - 31 December 2019\nTime step: hourly\nDepths: -2.35 m\n\nOnce the extraction request was submitted the dataset was created after an one hour of processing the data was available for download from Extraction request: Example dataset: Wind-vs-Current at Davies and Myrmidon Reefs (2019).\n\n\nDownloading the data\nIn this notebook we will download the data using scripting. There is no need to re-run the extraction request as each extraction performed by the extraction tool has a permanent public page created for it that can be used to facilitate sharing of the data.\nLet’s first create a temporary folder to contain the downloaded data. Note: The temp folder is excluded using the .gitignore so it is not saved to the code repository, which is why we must reproduce it.\n\nimport os\nif not os.path.exists('temp'):\n    os.makedirs('temp')\n\nNow let’s download the data. The file to download is 12.9 MB and so this download might take a little while. To allow us to re-run this script without having to wait for the download each time we first check that the download has not already been done.\n\nimport urllib.request\nextractionfile = os.path.join('temp','2009.c451dc3-collected.csv')  # Use os.path.join so the script will work cross-platform\n\nif not os.path.exists(extractionfile):\n    print(\"Downloading extraction data ...\")\n    url = 'https://api.ereefs.aims.gov.au/data-extraction/request/2009.c451dc3/files/2009.c451dc3-collected.csv'\n    req = urllib.request.urlretrieve(url, extractionfile)\n    print(req)\nelse:\n    print(\"Skipping redownloading extraction data\")\n\nSkipping redownloading extraction data\n\n\n\n\nReading and cleaning the data\nRead the resulting CSV file into a Pandas data frame.\n\nimport pandas as pd\ndf = pd.read_csv(extractionfile)\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Aggregated Date/Time\n      Variable\n      Depth\n      Site Name\n      Latitude\n      Longitude\n      mean\n      median\n      p5\n      p95\n      lowest\n      highest\n    \n  \n  \n    \n      0\n      2019-01-01T00:00\n      wspeed_u\n      99999.90\n      Myrmidon Reef\n      -18.265599\n      147.389028\n      -9.568488\n      -9.568488\n      -9.568488\n      -9.568488\n      -9.568488\n      -9.568488\n    \n    \n      1\n      2019-01-01T00:00\n      wspeed_u\n      99999.90\n      Davies Reef\n      -18.822840\n      147.645209\n      -8.880175\n      -8.880175\n      -8.880175\n      -8.880175\n      -8.880175\n      -8.880175\n    \n    \n      2\n      2019-01-01T00:00\n      wspeed_v\n      99999.90\n      Myrmidon Reef\n      -18.265599\n      147.389028\n      3.260430\n      3.260430\n      3.260430\n      3.260430\n      3.260430\n      3.260430\n    \n    \n      3\n      2019-01-01T00:00\n      wspeed_v\n      99999.90\n      Davies Reef\n      -18.822840\n      147.645209\n      2.756750\n      2.756750\n      2.756750\n      2.756750\n      2.756750\n      2.756750\n    \n    \n      4\n      2019-01-01T00:00\n      v\n      -2.35\n      Myrmidon Reef\n      -18.265599\n      147.389028\n      0.047311\n      0.047311\n      0.047311\n      0.047311\n      0.047311\n      0.047311\n    \n  \n\n\n\n\nHere we can see that data is in tidy format. That is each observation is in one row per time step, variable, depth and site. For each of these combinations there are summary statistics corresponding to the aggregation process that was applied. Somewhat confusingly we extracted hourly data from the hourly eReefs hydrodynamic model and so there was no aggregation applied. As a result all the stats values are the same and the mean corresponds to the actual extracted data values. Let’s clean this up a little by removing the redundant columns. Here we also drop the depth variable as the depth of the wind doesn’t match the depth of the surface current and so makes it difficult to align the wind and the current together. Since we are only processing a single depth this is an OK simplification. If we were processing the current at multiple depths then we would need a more complex set of data wraggling.\n\ndf2 = df.drop(columns=['median', 'p5', 'p95', 'lowest','highest','Depth']).rename(columns={\"mean\": \"value\"})\ndf2.head()\n\n\n\n\n\n  \n    \n      \n      Aggregated Date/Time\n      Variable\n      Site Name\n      Latitude\n      Longitude\n      value\n    \n  \n  \n    \n      0\n      2019-01-01T00:00\n      wspeed_u\n      Myrmidon Reef\n      -18.265599\n      147.389028\n      -9.568488\n    \n    \n      1\n      2019-01-01T00:00\n      wspeed_u\n      Davies Reef\n      -18.822840\n      147.645209\n      -8.880175\n    \n    \n      2\n      2019-01-01T00:00\n      wspeed_v\n      Myrmidon Reef\n      -18.265599\n      147.389028\n      3.260430\n    \n    \n      3\n      2019-01-01T00:00\n      wspeed_v\n      Davies Reef\n      -18.822840\n      147.645209\n      2.756750\n    \n    \n      4\n      2019-01-01T00:00\n      v\n      Myrmidon Reef\n      -18.265599\n      147.389028\n      0.047311\n    \n  \n\n\n\n\nNext we need to align the wind and current values for a given time step and site on to the same rows of data. Here we are converting the data from tall format to wide format.\n\ndf3 = df2.pivot(index = [\"Site Name\", \"Latitude\", \"Longitude\", \"Aggregated Date/Time\", ], columns=\"Variable\", values=\"value\")\ndf3.head()\n\n\n\n\n\n  \n    \n      \n      \n      \n      Variable\n      u\n      v\n      wspeed_u\n      wspeed_v\n    \n    \n      Site Name\n      Latitude\n      Longitude\n      Aggregated Date/Time\n      \n      \n      \n      \n    \n  \n  \n    \n      Davies Reef\n      -18.82284\n      147.645209\n      2019-01-01T00:00\n      -0.048835\n      0.100391\n      -8.880175\n      2.756750\n    \n    \n      2019-01-01T01:00\n      -0.090807\n      -0.036129\n      -8.749271\n      2.753487\n    \n    \n      2019-01-01T02:00\n      -0.118289\n      -0.180480\n      -8.463824\n      2.587230\n    \n    \n      2019-01-01T03:00\n      -0.110750\n      -0.278911\n      -8.223801\n      2.112059\n    \n    \n      2019-01-01T04:00\n      -0.079472\n      -0.312360\n      -8.565171\n      2.711215\n    \n  \n\n\n\n\nOur aim is to create an index that estimates the correlation of the current and the wind vectors.\nThe correlation of the current and wind vectors can be estimated based using the dot product. An overview of the relationship between correlation and using the dot product is described in Geometric Interpretation of the Correlation between Two Variables. The correlation between the two vectors is given by:\n\\[\nr = \\cos(\\theta) = \\frac{a \\cdot b}{||a||\\cdot||b||}\n\\]\nwhere \\(a \\cdot b\\) is the dot product between the two vectors and \\(||a||\\) and \\(||b||\\) are the magnitudes of the vectors. The dot product can be calculated as\n\\[\na \\cdot b = a_x \\times b_x + a_y \\times b_y\n\\]\nand the magnitude of the vectors as\n\\[\n||a|| = \\sqrt{a^2_x + a^2_y} \\;\\;, \\; \\;\\;\\;\\;\n||b|| = \\sqrt{b^2_x + b^2_y}\n\\]\n\nimport numpy as np\ndf3['currentmag'] = np.sqrt(df3['u']**2+df3['v']**2)\ndf3['windmag'] = np.sqrt(df3['wspeed_u']**2+df3['wspeed_v']**2)\ndf3['windcurrentcorr'] = (df3['u'] * df3['wspeed_u'] + df3['v'] * df3['wspeed_v'])/(df3['currentmag']*df3['windmag'])\ndf3.head()\n\n\n\n\n\n  \n    \n      \n      \n      \n      Variable\n      u\n      v\n      wspeed_u\n      wspeed_v\n      currentmag\n      windmag\n      windcurrentcorr\n    \n    \n      Site Name\n      Latitude\n      Longitude\n      Aggregated Date/Time\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Davies Reef\n      -18.82284\n      147.645209\n      2019-01-01T00:00\n      -0.048835\n      0.100391\n      -8.880175\n      2.756750\n      0.111638\n      9.298236\n      0.684380\n    \n    \n      2019-01-01T01:00\n      -0.090807\n      -0.036129\n      -8.749271\n      2.753487\n      0.097730\n      9.172319\n      0.775325\n    \n    \n      2019-01-01T02:00\n      -0.118289\n      -0.180480\n      -8.463824\n      2.587230\n      0.215790\n      8.850428\n      0.279727\n    \n    \n      2019-01-01T03:00\n      -0.110750\n      -0.278911\n      -8.223801\n      2.112059\n      0.300094\n      8.490683\n      0.126259\n    \n    \n      2019-01-01T04:00\n      -0.079472\n      -0.312360\n      -8.565171\n      2.711215\n      0.322311\n      8.984033\n      -0.057391\n    \n  \n\n\n\n\nLet’s look at the relationship between the wind and current as a function of the wind speed. Here we are considering each hourly sample as an independent estimate of the relationship. In reality this is not the case as the longer the wind blows the more effect it will have on the current. As this is just a coding example and not an in-depth analysis we don’t need to worry about this limitation of the analysis.\nLet’s pull out the data for Davies and Myrmidon Reefs separately so they are easy to plot.\n\ndavies = df3.query('`Site Name` == \"Davies Reef\"')\nmyrmidon = df3.query('`Site Name` == \"Myrmidon Reef\"')\nmyrmidon.head()\n\n\n\n\n\n  \n    \n      \n      \n      \n      Variable\n      u\n      v\n      wspeed_u\n      wspeed_v\n      currentmag\n      windmag\n      windcurrentcorr\n    \n    \n      Site Name\n      Latitude\n      Longitude\n      Aggregated Date/Time\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Myrmidon Reef\n      -18.265599\n      147.389028\n      2019-01-01T00:00\n      0.012498\n      0.047311\n      -9.568488\n      3.260430\n      0.048934\n      10.108727\n      0.070078\n    \n    \n      2019-01-01T01:00\n      -0.029212\n      -0.013297\n      -9.420339\n      3.528335\n      0.032096\n      10.059420\n      0.707019\n    \n    \n      2019-01-01T02:00\n      -0.060520\n      -0.066542\n      -9.333500\n      3.765564\n      0.089947\n      10.064477\n      0.347188\n    \n    \n      2019-01-01T03:00\n      -0.078321\n      -0.111699\n      -9.281394\n      3.784521\n      0.136421\n      10.023316\n      0.222466\n    \n    \n      2019-01-01T04:00\n      -0.076759\n      -0.132760\n      -8.527690\n      3.885759\n      0.153353\n      9.371266\n      0.096513\n    \n  \n\n\n\n\nLet’s create a scatter plot to see if there is a relationship between the wind and currents.\n\nimport matplotlib.pyplot as plt\nfig = plt.figure()\nax=fig.add_axes([0,0,1,1])\nax.scatter(myrmidon[\"windmag\"],myrmidon[\"windcurrentcorr\"], color='r', s=1)\nax.scatter(davies[\"windmag\"],davies[\"windcurrentcorr\"], color='b', s=1)\nax.set_xlabel('Wind speed (m/s)')\nax.set_ylabel('Wind-current correlation')\nax.set_title('Correlation between wind and surface current (hourly data, 2019)')\n\nText(0.5, 1.0, 'Correlation between wind and surface current (hourly data, 2019)')\n\n\n\n\n\nThis scatter plot shows that the relationship between wind and current is weak. This is not surprising given that we are considering just the hourly samples, with no consideration for how long the wind has been blowing. At low wind conditions the current has an even chance of being aligned with the wind (correlation \\(r= 1\\)) as in the opposite direction (correlation \\(r= -1\\)), however in high wind we can see that there is much more chance that the currents are aligned with the wind.\nTo understand this relationship better we want to understand how much the wind nudges the current in its direction. If we bin the wind speeds then collect all the correlation samples in each bin then we can see if they average to zero (indicating that there is no relationship between the wind and current) or there is average alignment.\n\nfrom scipy import stats\nwind = davies[\"windmag\"]\ncurrent = davies[\"windcurrentcorr\"]\nbin_means, bin_edges, binnumber = stats.binned_statistic(wind, current, 'mean', bins=20)\nplt.hlines(bin_means, bin_edges[:-1], bin_edges[1:], colors='g', lw=5,\n           label='Davies Reef')\n\nwind = myrmidon[\"windmag\"]\ncurrent = myrmidon[\"windcurrentcorr\"]\nbin_means, bin_edges, binnumber = stats.binned_statistic(wind, current, 'mean', bins=20)\nplt.hlines(bin_means, bin_edges[:-1], bin_edges[1:], colors='r', lw=5,\n           label='Myrmidon Reef')\n\nplt.xlabel('Wind speed (m/s)')\nplt.ylabel('Wind-current correlation')\nplt.title('Mean correlation between wind and surface current (hourly data, 2019)')\nplt.legend()\n\n<matplotlib.legend.Legend at 0x20af5ae26d0>\n\n\n\n\n\nFrom this we can see that for wind speeds below 8 m/s the surface current direction is unrelated to the wind. Above this wind speed the surface current is increasingly determined by the direction of the wind. By the time the wind is 16 m/s the direction of the surface current is dominated by the wind direction.\n\nNote: It should be remembered that this analysis is based on the eReefs Hydrodynamic model and as such is not based on real data. The eReefs model has however been tuned to accurately capture the flow dynamics of the GBR and so we would expect the estimates from this analysis to be approximately correct."
  },
  {
    "objectID": "tutorials/python/python_tutorials_homepage.html",
    "href": "tutorials/python/python_tutorials_homepage.html",
    "title": "AIMS eReefs tutorials",
    "section": "",
    "text": "This is the sub-homepage for the Python tutorials."
  },
  {
    "objectID": "tutorials/python/server-data-access-python/server_data_access_python.html",
    "href": "tutorials/python/server-data-access-python/server_data_access_python.html",
    "title": "Accessing eReefs data from the AIMS server  Python",
    "section": "",
    "text": "First of all, load the necessary libraries.\n\nfrom netCDF4 import Dataset, num2date\nimport matplotlib.pyplot as plt\nimport cartopy\nimport cartopy.crs as ccrs\nimport os\ncartopy.config['data_dir'] = os.getenv('CARTOPY_DIR', cartopy.config.get('data_dir'))"
  },
  {
    "objectID": "tutorials/python/server-data-access-python/server_data_access_python.html#define-which-data-to-be-plotted.",
    "href": "tutorials/python/server-data-access-python/server_data_access_python.html#define-which-data-to-be-plotted.",
    "title": "Accessing eReefs data from the AIMS server  Python",
    "section": "Define which data to be plotted.",
    "text": "Define which data to be plotted.\nIn this section we define which data we want to read and plot.\n\ninputFile - The netCDF input file. This can either be a downloaded file (see How to manually download AIMS eReefs data) or a OPeNDAP URL from the AIMS THREDDS server. For this tutorial we are using the OPeNDAP URL for the file EREEFS_AIMS-CSIRO_gbr4_v2_hydro_daily-monthly-2010-09.nc.\nselectedVariable - The name of the variable in the netCDF file.\nselectedTimeIndex The time slice in the netCDF file. Note the index starts with 0. For example, in the netCDF file “EREEFS_AIMS-CSIRO_gbr4_v2_hydro_daily-monthly-2010-09.nc” the time steps are “days”. This means if you select selectedTimeIndex=1 it refers to the second day in this file, which is 02/09/2010. selectedDepthIndex The depth slice in the netCDF file. Note the index starts with 0. See the following table for a mapping of index to value:\n\n\n\n\n\n\n\n\n\nIndex (k)\nHydrodynamic 1km model\nHydrodynamic and BioGeoChemical 4km model\n\n\n\n\n0\n-140.00 m\n-145.00 m\n\n\n1\n-120.00 m\n-120.00 m\n\n\n2\n-103.00 m\n-103.00 m\n\n\n3\n-88.00 m\n-88.00 m\n\n\n4\n-73.00 m\n-73.00 m\n\n\n5\n-60.00 m\n-60.00 m\n\n\n6\n-49.00 m\n-49.00 m\n\n\n7\n-39.50 m\n-39.50 m\n\n\n8\n-31.00 m\n-31.00 m\n\n\n9\n-24.00 m\n-23.75 m\n\n\n10\n-18.00 m\n-17.75 m\n\n\n11\n-13.00 m\n-12.75 m\n\n\n12\n-9.00 m\n-8.80 m\n\n\n13\n-5.25 m\n-5.55 m\n\n\n14\n-2.35 m\n-3.00 m\n\n\n15\n-0.50 m\n-1.50 m\n\n\n16\nn/a\n-0.50 m\n\n\n\n\n# OPeNDAP URL to file \"EREEFS_AIMS-CSIRO_gbr4_v2_hydro_daily-monthly-2010-09.nc\". Hydrodynamic 4km model, daily data for the month September 2010\ninputFile = \"http://thredds.ereefs.aims.gov.au/thredds/dodsC/s3://aims-ereefs-public-prod/derived/ncaggregate/ereefs/gbr4_v2/daily-monthly/EREEFS_AIMS-CSIRO_gbr4_v2_hydro_daily-monthly-2010-09.nc\"\ninputFile\n\n'http://thredds.ereefs.aims.gov.au/thredds/dodsC/s3://aims-ereefs-public-prod/derived/ncaggregate/ereefs/gbr4_v2/daily-monthly/EREEFS_AIMS-CSIRO_gbr4_v2_hydro_daily-monthly-2010-09.nc'\n\n\n\n# The \"temperature\" variable\nselectedVariable = \"temp\"\nselectedVariable\n\n'temp'\n\n\n\n# 2nd of September 2010. Note the index starts with 0\nselectedTimeIndex = 1\nselectedTimeIndex\n\n1\n\n\n\n# -1.50 m depth. Note the index starts with 0\nselectedDepthIndex = 15\nselectedDepthIndex\n\n15"
  },
  {
    "objectID": "tutorials/python/server-data-access-python/server_data_access_python.html#read-in-the-netcdf-file-contents",
    "href": "tutorials/python/server-data-access-python/server_data_access_python.html#read-in-the-netcdf-file-contents",
    "title": "Accessing eReefs data from the AIMS server  Python",
    "section": "Read in the netCDF file contents",
    "text": "Read in the netCDF file contents\nRead in the netCDF file contents and store the latitude, longitude and the variable data for the selected time and depth slice.\n\nnc_data = Dataset(inputFile, 'r')\nnc_data.title\n\n'eReefs AIMS-CSIRO GBR4 Hydrodynamic v2 daily aggregation'\n\n\n\nlons = nc_data.variables['longitude'][:]\nprint(lons[0:5]) # look at the first few entries in the longitude vector\n\n[142.168788 142.198788 142.228788 142.258788 142.288788]\n\n\n\nlats = nc_data.variables['latitude'][:]\nprint(lats[0:5]) # look at the first few entries in the latitude vector\n\n[-28.696022 -28.666022 -28.636022 -28.606022 -28.576022]\n\n\n\nvariableData = nc_data.variables[selectedVariable][selectedTimeIndex, selectedDepthIndex, :, :]\nprint(variableData[0:5]) # look at the first few entries in the variable data\n\n[[      nan       nan       nan ...       nan       nan       nan]\n [      nan       nan       nan ... 19.706053 19.704573 19.697897]\n [      nan       nan       nan ... 19.70424  19.70451  19.699736]\n [      nan       nan       nan ... 19.700645 19.705711 19.70135 ]\n [      nan       nan       nan ... 19.69414  19.70423  19.703842]]"
  },
  {
    "objectID": "tutorials/python/server-data-access-python/server_data_access_python.html#plotting",
    "href": "tutorials/python/server-data-access-python/server_data_access_python.html#plotting",
    "title": "Accessing eReefs data from the AIMS server  Python",
    "section": "Plotting",
    "text": "Plotting\nFinally, plot the data.\n\nplt.figure(figsize=(12, 8), dpi=80, facecolor='w', edgecolor='k')\nax = plt.axes(projection=ccrs.PlateCarree())\nax.set_extent([135, 165, -6.5, -29.5], ccrs.PlateCarree())\n\nplt.contourf(lons, lats, variableData, 30, transform=ccrs.PlateCarree())\nax.coastlines()\n\nplt.show()"
  },
  {
    "objectID": "tutorials/r/r_tutorials_homepage.html",
    "href": "tutorials/r/r_tutorials_homepage.html",
    "title": "AIMS eReefs tutorials",
    "section": "",
    "text": "This is the sub-home page for the R tutorials."
  },
  {
    "objectID": "tutorials/r/server-data-access-r/server_data_access_r.html",
    "href": "tutorials/r/server-data-access-r/server_data_access_r.html",
    "title": "Accessing eReefs data from the AIMS server ",
    "section": "",
    "text": "In this tutorial we will look at how to access eReefs data directly from the AIMS THREDDS server in R. This server hosts both ?raw and aggregated? eReefs model data in netCDF file format and offers access to the data files via OPeNDAP, HTTP Server, and Web Map Service (WMS). While we could download the data files manually via the HTTPServer link, this becomes cumbersome when downloading multiple files, given their large size. Thankfully, OPeNDAP provides a way to access the data files over the internet and extract only the data we want.\nFor example, say we want the daily mean surface temperature at a single location for the last 30 days. If we were to download the 30 individual daily aggregated netCDF files, with each file ~ 350 Mb, this would require us to download over 10 Gb of data just to get 300 numbers. The vast majority of this data would be irrelevant to our needs as the netCDF files contain data for a range of variables, at a range of depths, for many, many locations. However, with OPeNDAP, we can extract the daily mean values directly from the server without downloading any superfluous data.\n\nNavigating the eReefs server\nADD: Info about the folder and file naming on the AIMS THREDDS Server.\n\n\nR packages\n\nlibrary(RNetCDF) # working with netcdf files (incl. via OPeNDAP)\nlibrary(raster) # creating and manipuling rasters\nlibrary(rgdal) # package for geospatial analysis\n\n\n\n\n\n\n\nWhile the ncdf4 package is commonly used to work with NetCDF files in R, it does not offer compatibility with OPeNDAP for Windows (only Mac and Linux). For this reason we will use the RNetCDF package which offers similar functionality and Windows compatibility with OPeNDAP. Note that if you are using Mac or Linux and wish to use ncdf4, the functions used herein have obvious analogues; for example ncdf4::nc_open() vs. RNetCDF::open.nc().\n\n\n\n\n\nExample 1: Basic server access\nIn this example we will extract the daily mean water temperature for the 10th of December 2022 at 1.5 m depth across the entire scope of the eReefs model. We will then produce a very basic plot of the data.\n\nConnect to a file on the server\nFirst we need to find the right NetCDF file on the server. The available eReefs data NetCDF files are listed in the AIMS THREDDS Server catalogue. We will navigate to the eReefs 4 km Hydrodynamic Model daily aggregated data for the month of December 2022 and copy the OPeNDAP data URL.\n\n\ninput_file <- \"https://thredds.ereefs.aims.gov.au/thredds/dodsC/s3://aims-ereefs-public-prod/derived/ncaggregate/ereefs/gbr4_v2/daily-monthly/EREEFS_AIMS-CSIRO_gbr4_v2_hydro_daily-monthly-2022-12.nc\"\n\nWe can then open a connection to this file using the RNetCDF::open.nc function.\n\ndailyAggDec22.nc <- open.nc(input_file)\n\n\n\n\n\n\n\nIf you wish to download NetCDF files from the server you can click the HTTPServer link instead of OPeNDAP. The file can then be loaded into R by specifying the path: open.nc(\"<path to downloaded file>\").\n\n\n\n\n\nPrint a summary of the file\nIf we do not know what variables or dimensions are available in the file we have connected to, we can print a summary.\n\nsummary <- print.nc(dailyAggDec22.nc)\n\n\n\nsummary\n\n\n\nnetcdf classic {\ndimensions:\n    time = UNLIMITED ; // (31 currently)\n    k = 17 ;\n    latitude = 723 ;\n    longitude = 491 ;\nvariables:\n    NC_FLOAT mean_cur(longitude, latitude, k, time) ;\n        NC_CHAR mean_cur:puv__parameter = \"http://vocab.nerc.ac.uk/collection/P01/current/LCEWMP01/\" ;\n        NC_CHAR mean_cur:coordinates = \"time zc latitude longitude\" ;\n        NC_CHAR mean_cur:units = \"ms-1\" ;\n        NC_CHAR mean_cur:short_name = \"mean_cur\" ;\n        NC_CHAR mean_cur:aggregation = \"mean_speed\" ;\n        NC_CHAR mean_cur:standard_name = \"mean_current_speed\" ;\n        NC_CHAR mean_cur:long_name = \"mean_current_speed\" ;\n        NC_INT mean_cur:_ChunkSizes = 1, 1, 133, 491 ;\n    NC_FLOAT salt(longitude, latitude, k, time) ;\n        NC_CHAR salt:qudt__unit = \"http://qudt.org/vocab/unit/PSU\" ;\n        NC_CHAR salt:puv__parameter = \"http://vocab.nerc.ac.uk/collection/P01/current/PSLTMP01/\" ;\n        NC_CHAR salt:coordinates = \"time zc latitude longitude\" ;\n        NC_CHAR salt:short_name = \"salt\" ;\n        NC_CHAR salt:aggregation = \"Daily\" ;\n        NC_CHAR salt:units = \"PSU\" ;\n        NC_CHAR salt:long_name = \"Salinity\" ;\n        NC_INT salt:_ChunkSizes = 1, 1, 133, 491 ;\n    NC_FLOAT temp(longitude, latitude, k, time) ;\n        NC_CHAR temp:puv__parameter = \"https://vocab.nerc.ac.uk/collection/P01/current/TEMPMP01/\" ;\n        NC_CHAR temp:coordinates = \"time zc latitude longitude\" ;\n        NC_CHAR temp:short_name = \"temp\" ;\n        NC_CHAR temp:aggregation = \"Daily\" ;\n        NC_CHAR temp:units = \"degrees C\" ;\n        NC_CHAR temp:long_name = \"Temperature\" ;\n        NC_INT temp:_ChunkSizes = 1, 1, 133, 491 ;\n    NC_FLOAT u(longitude, latitude, k, time) ;\n        NC_CHAR u:vector_components = \"u v\" ;\n        NC_CHAR u:puv__parameter = \"http://vocab.nerc.ac.uk/collection/P01/current/LCEWMP01/\" ;\n        NC_CHAR u:coordinates = \"time zc latitude longitude\" ;\n        NC_CHAR u:short_name = \"u\" ;\n        NC_CHAR u:standard_name = \"eastward_sea_water_velocity\" ;\n        NC_CHAR u:vector_name = \"Currents\" ;\n        NC_CHAR u:aggregation = \"Daily\" ;\n        NC_CHAR u:units = \"ms-1\" ;\n        NC_CHAR u:long_name = \"Eastward current\" ;\n        NC_INT u:_ChunkSizes = 1, 1, 133, 491 ;\n    NC_FLOAT v(longitude, latitude, k, time) ;\n        NC_CHAR v:vector_components = \"u v\" ;\n        NC_CHAR v:puv__parameter = \"http://vocab.nerc.ac.uk/collection/P01/current/LCNSMP01/\" ;\n        NC_CHAR v:coordinates = \"time zc latitude longitude\" ;\n        NC_CHAR v:short_name = \"v\" ;\n        NC_CHAR v:standard_name = \"northward_sea_water_velocity\" ;\n        NC_CHAR v:vector_name = \"Currents\" ;\n        NC_CHAR v:aggregation = \"Daily\" ;\n        NC_CHAR v:units = \"ms-1\" ;\n        NC_CHAR v:long_name = \"Northward current\" ;\n        NC_INT v:_ChunkSizes = 1, 1, 133, 491 ;\n    NC_DOUBLE zc(k) ;\n        NC_CHAR zc:units = \"m\" ;\n        NC_CHAR zc:positive = \"up\" ;\n        NC_CHAR zc:long_name = \"Z coordinate\" ;\n        NC_CHAR zc:axis = \"Z\" ;\n        NC_CHAR zc:coordinate_type = \"Z\" ;\n        NC_CHAR zc:_CoordinateAxisType = \"Height\" ;\n        NC_CHAR zc:_CoordinateZisPositive = \"up\" ;\n    NC_DOUBLE time(time) ;\n        NC_CHAR time:units = \"days since 1990-01-01 00:00:00 +10\" ;\n        NC_CHAR time:long_name = \"Time\" ;\n        NC_CHAR time:standard_name = \"time\" ;\n        NC_CHAR time:coordinate_type = \"time\" ;\n        NC_CHAR time:puv__uom = \"http://vocab.nerc.ac.uk/collection/P06/current/UTAA/\" ;\n        NC_CHAR time:calendar = \"gregorian\" ;\n        NC_CHAR time:_CoordinateAxisType = \"Time\" ;\n        NC_INT time:_ChunkSizes = 1024 ;\n    NC_DOUBLE latitude(latitude) ;\n        NC_CHAR latitude:units = \"degrees_north\" ;\n        NC_CHAR latitude:long_name = \"Latitude\" ;\n        NC_CHAR latitude:standard_name = \"latitude\" ;\n        NC_CHAR latitude:coordinate_type = \"latitude\" ;\n        NC_CHAR latitude:projection = \"geographic\" ;\n        NC_CHAR latitude:puv__ofProperty = \"http://vocab.nerc.ac.uk/collection/S06/current/S0600045/\" ;\n        NC_CHAR latitude:puv__uom = \"http://vocab.nerc.ac.uk/collection/P06/current/DEGN/\" ;\n        NC_CHAR latitude:_CoordinateAxisType = \"Lat\" ;\n    NC_DOUBLE longitude(longitude) ;\n        NC_CHAR longitude:puv__uom = \"http://vocab.nerc.ac.uk/collection/P06/current/DEGE/\" ;\n        NC_CHAR longitude:units = \"degrees_east\" ;\n        NC_CHAR longitude:long_name = \"Longitude\" ;\n        NC_CHAR longitude:standard_name = \"longitude\" ;\n        NC_CHAR longitude:coordinate_type = \"longitude\" ;\n        NC_CHAR longitude:projection = \"geographic\" ;\n        NC_CHAR longitude:_CoordinateAxisType = \"Lon\" ;\n    NC_FLOAT mean_wspeed(longitude, latitude, time) ;\n        NC_CHAR mean_wspeed:puv__parameter = \"http://vocab.nerc.ac.uk/collection/P01/current/ESEWMPXX/\" ;\n        NC_CHAR mean_wspeed:coordinates = \"time latitude longitude\" ;\n        NC_CHAR mean_wspeed:units = \"ms-1\" ;\n        NC_CHAR mean_wspeed:short_name = \"mean_wspeed\" ;\n        NC_CHAR mean_wspeed:aggregation = \"mean_speed\" ;\n        NC_CHAR mean_wspeed:standard_name = \"mean_wind_speed\" ;\n        NC_CHAR mean_wspeed:long_name = \"mean_wind_speed\" ;\n        NC_INT mean_wspeed:_ChunkSizes = 1, 133, 491 ;\n    NC_FLOAT eta(longitude, latitude, time) ;\n        NC_CHAR eta:puv__parameter = \"https://vocab.nerc.ac.uk/collection/P01/current/ASLVMP01/\" ;\n        NC_CHAR eta:coordinates = \"time latitude longitude\" ;\n        NC_CHAR eta:short_name = \"eta\" ;\n        NC_CHAR eta:standard_name = \"sea_surface_height_above_geoid\" ;\n        NC_CHAR eta:aggregation = \"Daily\" ;\n        NC_CHAR eta:units = \"metre\" ;\n        NC_CHAR eta:positive = \"up\" ;\n        NC_CHAR eta:long_name = \"Surface elevation\" ;\n        NC_INT eta:_ChunkSizes = 1, 133, 491 ;\n    NC_FLOAT wspeed_u(longitude, latitude, time) ;\n        NC_CHAR wspeed_u:puv__parameter = \"http://vocab.nerc.ac.uk/collection/P01/current/ESEWMPXX/\" ;\n        NC_CHAR wspeed_u:coordinates = \"time latitude longitude\" ;\n        NC_CHAR wspeed_u:short_name = \"wspeed_u\" ;\n        NC_CHAR wspeed_u:aggregation = \"Daily\" ;\n        NC_CHAR wspeed_u:units = \"ms-1\" ;\n        NC_CHAR wspeed_u:long_name = \"eastward_wind\" ;\n        NC_INT wspeed_u:_ChunkSizes = 1, 133, 491 ;\n    NC_FLOAT wspeed_v(longitude, latitude, time) ;\n        NC_CHAR wspeed_v:puv__parameter = \"http://vocab.nerc.ac.uk/collection/P01/current/ESNSMPXX/\" ;\n        NC_CHAR wspeed_v:coordinates = \"time latitude longitude\" ;\n        NC_CHAR wspeed_v:short_name = \"wspeed_v\" ;\n        NC_CHAR wspeed_v:aggregation = \"Daily\" ;\n        NC_CHAR wspeed_v:units = \"ms-1\" ;\n        NC_CHAR wspeed_v:long_name = \"northward_wind\" ;\n        NC_INT wspeed_v:_ChunkSizes = 1, 133, 491 ;\n\n// global attributes:\n        NC_CHAR :Conventions = \"CF-1.0\" ;\n        NC_CHAR :Parameter_File_Revision = \"$Revision: 1753 $\" ;\n        NC_CHAR :Run_ID = \"2.1\" ;\n        NC_CHAR :Run_code = \"GBR4 Hydro|G0.00|H2.10|S0.00|B0.00\" ;\n        NC_CHAR :_CoordSysBuilder = \"ucar.nc2.dataset.conv.CF1Convention\" ;\n        NC_CHAR :aims_ncaggregate_buildDate = \"2023-01-25T03:48:20+10:00\" ;\n        NC_CHAR :aims_ncaggregate_datasetId = \"products__ncaggregate__ereefs__gbr4_v2__daily-monthly/EREEFS_AIMS-CSIRO_gbr4_v2_hydro_daily-monthly-2022-12\" ;\n        NC_CHAR :aims_ncaggregate_firstDate = \"2022-12-01T00:00:00+10:00\" ;\n        NC_CHAR :aims_ncaggregate_inputs = \"[products__ncaggregate__ereefs__gbr4_v2__raw/EREEFS_AIMS-CSIRO_gbr4_v2_hydro_raw_2022-12::MD5:bd27b033d40e598c20348044720deb73]\" ;\n        NC_CHAR :aims_ncaggregate_lastDate = \"2022-12-31T00:00:00+10:00\" ;\n        NC_CHAR :bald__isPrefixedBy = \"prefix_list\" ;\n        NC_CHAR :date_created = \"Sun Dec 11 21:24:39 2022\" ;\n        NC_CHAR :description = \"Aggregation of raw hourly input data (from eReefs AIMS-CSIRO GBR4 Hydrodynamic v2 subset) to daily means. Also calculates mean magnitude of wind and ocean current speeds. Data is regridded from curvilinear (per input data) to rectilinear via inverse weighted distance from up to 4 closest cells.\" ;\n        NC_CHAR :ems_version = \"v1.4.0 rev(6949)\" ;\n        NC_CHAR :history = \"2023-01-24T10:49:00+10:00: vendor: AIMS; processing: None summaries\n2023-01-25T03:48:20+10:00: vendor: AIMS; processing: Daily summaries\" ;\n        NC_CHAR :metadata_link = \"https://eatlas.org.au/data/uuid/350aed53-ae0f-436e-9866-d34db7f04d2e\" ;\n        NC_CHAR :paramfile = \"./prm/gbr4_hydro_nrt.prm\" ;\n        NC_CHAR :paramhead = \"GBR 4km resolution grid\" ;\n        NC_CHAR :prefix_list_puv__ = \"https://w3id.org/env/puv#\" ;\n        NC_CHAR :prefix_list_qudt__ = \"http://qudt.org/vocab/unit/\" ;\n        NC_CHAR :technical_guide_link = \"https://eatlas.org.au/pydio/public/aims-ereefs-platform-technical-guide-to-derived-products-from-csiro-ereefs-models-pdf\" ;\n        NC_CHAR :technical_guide_publish_date = \"2020-08-18\" ;\n        NC_CHAR :title = \"eReefs AIMS-CSIRO GBR4 Hydrodynamic v2 daily aggregation\" ;\n        NC_CHAR :DODS_EXTRA.Unlimited_Dimension = \"time\" ;\n}\n\n\n\n\n\nExtract data from the server\nNow that we have an open connection to a file on the server we need to extract the daily mean temperature at 1.5m depth for the 10th of December.\nFrom the summary output above we can see that the variable corresponding to temperature is: \\(\\texttt{ temp(longitude, latitude, k, time)}\\).\nThe dimensions for temperature are in brackets. This means that there is a temperature value for every combination of longitude, latitude, depth (k) and time. We can now see why these NetCDF files are so large.\nTo extract the data we will use the function\nRNetCDF::var.get.nc(ncfile, variable, start=NA, count=NA, ...)\nWe need to give the function:\n\nncfile: a NetCDF file connection; in our case dailyAggDec22.nc.\nvariable: the name or id of the data variable we wish to extract; in our case \"temp\".\nstart: a vector of indices of where to start getting data, one for each dimension of the variable. Since we have \\(\\texttt{temp(longitude, latitude, k, time)}\\) we need to tell the function where to start getting data along each of the four dimensions.\ncount: similar to start, but specifying the number of temperature values to extract along each dimension.\n\nLet’s look at how to construct our start and count vectors.\n\n\n\n\n\n\nThe default values of start and count are NA, in which case all data for the given variable will be extracted.\n\n\n\nDepth: Starting with depth is easy because we have a constant value of interest (1.5 m). The index k corresponds to different depths as shown in the table below, where we see k=16 maps to a depth of 1.5 m.\n\n\nTable of eReefs depths corresponding to index k\n\n\n\n\n\n\n\n\n\nIndex (k)\nHydrodynamic 1km model\nHydrodynamic & BioGeoChemical 4km models\n\n\n\n\n1\n-140.00 m\n-145.00 m\n\n\n2\n-120.00 m\n-120.00 m\n\n\n3\n-103.00 m\n-103.00 m\n\n\n4\n-88.00 m\n-88.00 m\n\n\n5\n-73.00 m\n-73.00 m\n\n\n6\n-60.00 m\n-60.00 m\n\n\n7\n-49.00 m\n-49.00 m\n\n\n8\n-39.50 m\n-39.50 m\n\n\n9\n-31.00 m\n-31.00 m\n\n\n10\n-24.00 m\n-23.75 m\n\n\n11\n-18.00 m\n-17.75 m\n\n\n12\n-13.00 m\n-12.75 m\n\n\n13\n-9.00 m\n-8.80 m\n\n\n14\n-5.25 m\n-5.55 m\n\n\n15\n-2.35 m\n-3.00 m\n\n\n16\n-0.50 m\n-1.50 m\n\n\n17\nn/a\n-0.50 m\n\n\n\n\nTime: Since we have the daily aggregated data for December 2022, and are interested only in a single day (the 10th), time is also a constant value. From the summary output we can see we have 31 time indexs, these correspond to the day of the month, therefore we want time=10.\nLongitude and latitude: We want temperatures for every available longitude and latitude so we can plot the data across the entire spatial range of the eReefs model. Therefore we want to start at index 1 and count for the entire length of latitude and longitude. To get the lengths we could note the values from the summary output, where we see \\(\\texttt{longitude = 491}\\) and \\(\\texttt{latitude = 723}\\). However we could also get the lengths programatically.\n\nlon <- var.get.nc(dailyAggDec22.nc, \"longitude\")\nlat <- var.get.nc(dailyAggDec22.nc, \"latitude\")\ndata.frame(length(lon), length(lat))\n\n  length.lon. length.lat.\n1         491         723\n\n\n\n\n\n\n\n\nWithin the eReefs netCDF files, the dimensions \\(\\texttt{longitude, latitude, k, time}\\) have corresponding variables longitude, latitude, zc, time (see summary output). Note that we would extract the dimension \\(\\texttt{k}\\) variable with var.get.nc(..., variable = \"zc\").\n\n\n\nNow we are ready to construct our start and count vectors and extract the data.\n\n# SETUP START AND COUNT VECTORS\n# Recall the order of the dimensions: (lon, lat, k , time)\n# We start at lon=1, lat=1, k=16, time=10 and get temps for\n# every lon and lat while holding depth and time constant\n\nlon_st <- 1\nlat_st <- 1\ndepth_st <- 16  # index k = 16 --> depth = 1.5 m\ntime_st <- 10   # index time = 10 --> 10th day of month\n\nlon_ct <- length(lon) # get temps for all lons and lats\nlat_ct <- length(lat)\ntime_ct <- 1  # Hold time and depth constant\ndepth_ct <- 1\n\nstart_vector <- c(lon_st, lat_st, depth_st, time_st)  \ncount_vector <- c(lon_ct, lat_ct, time_ct, depth_ct)\n\n# EXTRACT DATA\ntemps_10Dec22_1p5m <- var.get.nc(\n  ncfile = dailyAggDec22.nc, \n  variable = \"temp\",\n  start = start_vector, \n  count = count_vector\n)\n\n# Get the size of our extracted data\ndims <- dim(temps_10Dec22_1p5m)\ndata.frame(nrows = dims[1], ncols = dims[2])\n\n  nrows ncols\n1   491   723\n\n\n\n\nClose connection to file\nNow that our extracted data is stored in memory, we should close the connection to the NetCDF file on the server.\n\nclose.nc(dailyAggDec22.nc)\n\n\n\nPlot the data\nSave the data in a raster. We also need to transpose and flip to orient the data correctly.\nADD: why do we need to flip and transpose? Explain CRS (same for all ereefs netcdfs?)\n\ntemps_raster <- temps_10Dec22_1p5m |>\n  t() |>   # transpose temps\n  raster(  # create raster\n    xmn = min(lon), xmx = max(lon), \n    ymn = min(lat), ymx = max(lat), \n    crs = CRS(\"+init=epsg:4326\")\n  ) |>\n  flip(direction = 'y') # flip the raster\n\ntemps_raster\n\nclass      : RasterLayer \ndimensions : 723, 491, 354993  (nrow, ncol, ncell)\nresolution : 0.0299389, 0.02995851  (x, y)\nextent     : 142.1688, 156.8688, -28.69602, -7.036022  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nsource     : memory\nnames      : layer \nvalues     : 22.08588, 32.48817  (min, max)\n\n\n\n\n\n\n\n\nIn the code chunk above we use a pipe |>. Pipes are very useful when passing a dataset through a sequence of functions. In the code above we take our extracted temps, transpose them, turn them into a raster, and then flip the raster. The final result is saved to temps_raster.\n\n\n\nNow we have our temperature data as a raster (in the correct orientation), producing a simple plot is easy.\n\nplot(temps_raster)\n\n\n\n\n\n\n\nExample (advanced)"
  },
  {
    "objectID": "tutorials/r/time-series-plot-r/time_series_plot_r.html#basic",
    "href": "tutorials/r/time-series-plot-r/time_series_plot_r.html#basic",
    "title": "Times Series Plots ",
    "section": "Basic",
    "text": "Basic\nSince we are interested in gaining some insight into the possibility of northern coral larvae migrating southward through our site, we should have a look at the north-south current velocity data.\n\ndata %>% \n  filter(variable == \"v\") %>% # select the current variable\n  ggplot(aes(x = aggregated_date_time, y = mean)) + # and plot the daily mean\n  geom_line(alpha=0.7) + # specify a line graph of the mean\n  geom_abline(slope = 0, intercept = 0, color = \"red\", linewidth = 1) + # add a line a y=0\n  scale_x_datetime(date_breaks = \"1 year\", date_labels = \"%Y\") + # show only years on x-axis\n  theme_bw(base_size=13) + \n  labs(x = \"Year\", y = \"Northward current velocity (m/s)\")\n\n\n\n\nFigure 3: Time series plot of daily mean northward current velocity.\n\n\n\n\nHere we see that there does appear to be a cyclical pattern to the northward current at our site, with southward currents (i.e. negative northward current) primarily in the wet season. This is great news, as this is when coral spawning occurs!\nHowever, in order to get a better idea of what is happening, we should have a closer look at the data for the coral spawning period of roughly October - January."
  },
  {
    "objectID": "tutorials/r/time-series-plot-r/time_series_plot_r.html#advanced",
    "href": "tutorials/r/time-series-plot-r/time_series_plot_r.html#advanced",
    "title": "Times Series Plots ",
    "section": "Advanced",
    "text": "Advanced\nWe wish to produce the plot below, looking at all our data in an approximate coral spawning season of October through January, for each of the years in our data.\n\n\n\n\n\nFigure 4: Times series plot of daily mean northward current velocity during October through January for each year 2010-11 to 2021-22.\n\n\n\n\nThe first thing we can note, from this plot, is that it is very cluttered, making it difficult to detect any clear trend in our data. However, before we investigate this further, lets look at how the plot was made.\nUnfortunately, there does not seem to be any out-of-the-box or intuitive ways to plot a given season (i.e. period less than 12 months) for multiple years in ggplot. However, we can hack a solution by writing a function called ggTS_byYear which creates a fake date variable where all the data is converted to be in the same year and then plots this fake date along the x-axis and groups the data based on the real date. This function is defined below.\nYou can copy and paste this function into your script if you would just like to get to using it straight away. However, it is also heavily commented should you wish to customise it further, troubleshoout an errors you are encountering when using it, or fix any unforeseen bugs which may be present.\nNote: if troubleshooting, consider using R function browser.\n\n########################################################################\n## PLOT TIME SERIES BY YEAR OVER A GIVEN PERIOD <= 12 MONTHS          ##\n## ------------------------------------------------------------------ ##\n## RETURNS: ggplot object (without geoms)                             ##\n## REQUIRES: ggplot, dplyr, magritter, lubridate                      ##\n## ------------------------------------------------------------------ ##\n## Example:                                                           ##\n##    salinity_time_series_plot <-                                    ##\n##      ggTS_byYear(                                                  ##\n##        data = eReefs_data,                                         ##\n##        date_col_name = date_time,                                  ##\n##        response_col_name = daily_mean_salinity,                    ##\n##        start_month = 6,                                            ##\n##        end_month = 5                                               ##\n##      ) +                                                           ##\n##      geom_line() +                                                 ##\n##      labs(y = \"Daily mean salinity\", x = \"Date\", colour = \"Year\")  ##\n##                                                                    ##\n## Warning: not designed for plot periods > 12 months                 ##\n##                                                                    ##\n## Function concept: fake year(s) is used to put data for all years   ##\n##                   on same x-axis, where as plot period denotes     ##\n##                   real year(s)pertaining to the data dates         ##\n##                                                                    ##\n## Note: x-axis major breaks are months, if a different period is     ##\n##       required, use the ggplot2::scale_x_datetime() function       ##\n########################################################################\nggTS_byYear <- function(\n    data, # dataframe with POSIX dates and continuous response\n    date_col_name, # the name of the dataframe column with the date variable to plot\n    response_col_name, # the name of the dataframe column with the response variable to plot\n    start_month = 1, # lower time series limit (default January)\n    end_month = 12, # upper time series limit (default December)\n    minor_breaks_period = \"1 day\" # the period for the graph's x-axis minor breaks (e.g. 1 week, 1 day)\n) {\n  # SETUP\n  require(ggplot2)  # for plotting\n  require(magrittr) # source of the pipe (%>%) function\n  require(dplyr)    # data manipulation\n  require(lubridate) # date handling\n  fake_year <- 0001 # fake year used to have all dates over same period (grouped by real year)\n  \n  # APPEND VARIABLES TO DATA FOR USE IN PLOTTING\n  data = data %>%  \n    mutate(\n      datetime = as_datetime({{date_col_name}}), # Ensure dates in POSIX format\n      year = year(datetime), # Create columns for real year and\n      month = month(datetime) # real month\n    )\n  \n  # THE CASE WHEN THE PLOT PERIOD IS WITHNIN A SINGLE CALENDER YEAR (e.g. June 2016 - Nov 2016)\n  if (start_month <= end_month) {\n    # Get x-axis breaks and labels:\n    plot_months = c(start_month:(end_month+1)) # vector of months to plot (including end_month)\n    plot_breaks = make_datetime(fake_year, plot_months) # x-axis major breaks at each month\n    # Assign data to plot periods and fake years and filter out data not needed:\n    data <- data %>% \n      mutate(\n        # Plot period is within the real year (e.g. June 2016 - October 2016) \n        plot_period_label = paste(year), # data for all months pertain to respective year\n        dummy_date = update(datetime, year = fake_year) # all data plotted over fake year (e.g. 0001)\n      ) %>% \n      filter(month >= start_month & month <= end_month)\n  }\n  \n  # THE CASE WHEN THE PLOT PERIOD IS SPREAD ACROSS TWO CALENDER YEARS (e.g. Nov 2016 - June 2017)\n  if (start_month > end_month) {\n    # Get x-axis breaks and labels\n    plot_months_y1 = c(start_month:12) # a vector of months to plot in the former year\n    plot_months_y2 = c(1:(end_month+1)) # a vector of months to plot in the latter year\n    plot_months = c(plot_months_y1, plot_months_y2)\n    plot_breaks <- c(\n      make_datetime(fake_year, plot_months_y1),\n      make_datetime(fake_year+1, plot_months_y2)\n    ) \n    # Assign data to plot periods (i.e. based on real dates), create the fake date \n    # (using fake_year and fake_year +1), and filter out data not needed:\n    data <- data %>% \n      mutate(\n        # Plot period crosses two calender years, therefore\n        # data for months prior to start_month pertain to preceding plot period\n        plot_period_start = ifelse(month >= start_month, year, year-1), \n        plot_period_end = plot_period_start+1,\n        plot_period_label = paste(plot_period_start, substr(plot_period_end, 3, 4), sep = '-'), \n        # Dummy dates: months after start_month plotted in fake year (e.g. 0001), months prior plotted in 0002\n        dummy_year = ifelse(month >= start_month, fake_year, fake_year + 1), \n        dummy_date = update(datetime, year = dummy_year)\n      ) %>% \n      filter(month >= start_month | month <= end_month)\n  }\n  \n  # CREATE X-AXIS (DATES) BREAK LABELS \n  # If end_month is 12 (December), plot_months ends at 13 (January of next year)\n  plot_months <- replace(plot_months, plot_months==13, 1) # Change 13 to 1\n  break_labels <- month.abb[plot_months]\n\n  # CREATE PLOT\n  ts_plot <- data %>% \n    ggplot(aes(x = dummy_date, y = {{response_col_name}}, group = plot_period_label, colour = plot_period_label)) + \n    labs(x = \"Date\", y = \"Response\",  colour = \"Year\") + \n    theme_bw() + \n    scale_x_datetime(breaks = plot_breaks, labels = break_labels, date_minor_breaks = minor_breaks_period)\n\n  return(ts_plot)\n}\n\nNow we have our function definition, lets see it in action. Recall that our previous plot was quite cluttered, which made it difficult to discern any trends in the data. So this time, let’s plot for just the month of October.\n\ndata %>% \n  filter(variable == \"v\") %>%\n  ggTS_byYear(aggregated_date_time, mean, start_month = 10, end_month = 10) + \n  geom_line(linewidth = 0.7) + \n  geom_abline(slope = 0, intercept = 0, linewidth = 0.8) + \n  labs(y = \"Northward current velocity (m/s)\")\n\n\n\n\nFigure 5: Times series plot of daily mean northward current velocity during October for each year 2010 to 2021.\n\n\n\n\nThis is significantly easier to digest. However, there is still a lot going on, and sometimes we may wish to see more than a single month at a time."
  }
]